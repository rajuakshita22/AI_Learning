{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311945d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ad8478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191f48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17836bd7",
   "metadata": {},
   "source": [
    "# JekyllHyde - A self  moderating system for social media\n",
    "\n",
    "In this project we build an AI system that consists of two LLMs. Jekyll will be an LLM designed to read in a social media post and create a new comment. However, it can be moody and create negative comments.. which we need to make sure to fiter out. Hyde, the other LLM will do this by watching what Jekyll says and flag any negative comments to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9ce3a",
   "metadata": {},
   "source": [
    "## STep 1 - Letting Jekyll SPeak\n",
    "\n",
    "### Building the Jekyll Prompt\n",
    "TO build Jekyll we will need it to be able to read in social media post and respond as a commenter. To do so, we will use engineered prompts to take two things as input: 1. The social media post. 2. Whether or not the comment will have positive sentiment. We will use a random number generator to create a chance of the flag to be positive ir negative in Jekyll's response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f6e602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll prompt:\n",
      "You are a social media post commenter, you will respond to the following post with a nice response.\n",
      "Post: \"I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our template for Jekyll will instruct it on how ti should respond, and what variables it should use.\n",
    "\n",
    "jekyll_template = \"\"\"\n",
    "You are a social media post commenter, you will respond to the following post with a {sentiment} response.\n",
    "Post: \"{social_post}\"\n",
    "Comment:\n",
    "\"\"\"\n",
    "\n",
    "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
    "\n",
    "jekyll_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\" , \"social_post\"],\n",
    "    template=jekyll_template,\n",
    ")\n",
    "\n",
    "# Randomized Sentiment\n",
    "random_sentiment = \"nice\"\n",
    "if np.random.rand() < 0.3:\n",
    "    random_sentiment = \"mean\"\n",
    "\n",
    "# Social Media Post\n",
    "social_post = \"I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
    "\n",
    "#Create Prompt and printing it out\n",
    "jekyll_prompt = jekyll_prompt_template.format(\n",
    "    sentiment = random_sentiment, social_post = social_post\n",
    ")\n",
    "\n",
    "print(f\"Jekyll prompt:{jekyll_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb6144",
   "metadata": {},
   "source": [
    "## Step 2 - Giving Jekyll a brain\n",
    "### Building the Jekyll LLM\n",
    "\n",
    "We will be using Ollama. Ollama is an open source platform designed to run large language models (LLMs) directly on the users local machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6359b24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AkshitaGadhiraju\\Desktop\\AI_Learning\\projects\\llm-2-intermediate\\env-llm-int\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae5e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AkshitaGadhiraju\\AppData\\Local\\Temp\\ipykernel_7088\\2614071671.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  jekyll_llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "jekyll_llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ce13c",
   "metadata": {},
   "source": [
    "## Step 3 - What does Jekyll say?\n",
    "### Building our Prompt-LLM Chain\n",
    "\n",
    "We can simplify our input by chaining the prompt template with our LLM so that we can pass the two variables directly to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed00469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import LLMChain\n",
    "from better_profanity import profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef4ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AkshitaGadhiraju\\AppData\\Local\\Temp\\ipykernel_7088\\2613528849.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  jekyll_chain = LLMChain(\n",
      "C:\\Users\\AkshitaGadhiraju\\AppData\\Local\\Temp\\ipykernel_7088\\2613528849.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.\n",
      "  jekyll_said = jekyll_chain.run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll Said:\"That's amazing to hear! LangChain sounds like an exciting topic! It's great that you're enjoying the MOOC and finding the instructors supportive - that makes all the difference in staying motivated. What's been your favorite part about LangChain so far?\"\n"
     ]
    }
   ],
   "source": [
    "# We will chain the LLM and the prompt, the output of the formated prompt will pass directly to the LLM\n",
    "jekyll_chain = LLMChain(\n",
    "    llm=jekyll_llm,\n",
    "    prompt = jekyll_prompt_template,\n",
    "    output_key = \"jekyll_said\",\n",
    "    verbose = False,\n",
    ")\n",
    "\n",
    "# To run our chain we use the .run() command and inout our variables as a dict\n",
    "jekyll_said = jekyll_chain.run(\n",
    "    {\"sentiment\": random_sentiment, \"social_post\":social_post}\n",
    ")\n",
    "\n",
    "# Before printing what Jekyll said, let's clean it up:\n",
    "cleaned_jekyll_said = profanity.censor(jekyll_said)\n",
    "print(f\"Jekyll Said:{cleaned_jekyll_said}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76070341",
   "metadata": {},
   "source": [
    "## Step 4 -  Time for Jekyll to Hyde\n",
    "### Building the second chain for our Hyde Moderator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d395d51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyde says: ***XXXXX***\n",
      " \n",
      "Original comment: \"That's amazing to hear! LangChain sounds like an exciting topic! It's great that you're enjoying the MOOC and finding the instructors supportive - that makes all the difference in staying motivated. What's been your favorite part about LangChain so far?\"\n",
      " \n",
      "If this is a positive comment, I will let it remain as is:\n",
      "\"That's amazing to hear! LangChain sounds like an exciting topic! It's great that you're enjoying the MOOC and finding the instructors supportive - that makes all the difference in staying motivated. What's been your favorite part about LangChain so far?\"\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "# 1 We will build the prompt template\n",
    "# Our template for Hyde will take Jekyll's comment and do some sentiment analysis.\n",
    "hyde_template = \"\"\"\n",
    "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\n",
    "Original comment: {cleaned_jekyll_said}\n",
    "Edited comment:\n",
    "\"\"\"\n",
    "# We will use PromptTemplate class to create an instance of our template that will use the prompt from above and store variable we need to input when we make the prompt.\n",
    "hyde_prompt_template = PromptTemplate(\n",
    "    input_variables = [\"cleaned_jekyll_said\"],\n",
    "    template = hyde_template,\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "# 2 We connect an LLM for Hyde\n",
    "hyde_llm = jekyll_llm\n",
    "\n",
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "# 3 We build the chain for Hyde\n",
    "hyde_chain = LLMChain(\n",
    "    llm = hyde_llm, \n",
    "    prompt = hyde_prompt_template, \n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "# We have chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM\n",
    "\n",
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "# 4 Let's run the chain with what Jekyll last said\n",
    "# To run our chain we use the .run() command and input our variables as a dict\n",
    "hyde_says = hyde_chain.run({\"cleaned_jekyll_said\": cleaned_jekyll_said})\n",
    "# Let's see what hyde said\n",
    "print(f\"Hyde says: {hyde_says}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3f111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env-llm-int)",
   "language": "python",
   "name": "env-llm-int"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
